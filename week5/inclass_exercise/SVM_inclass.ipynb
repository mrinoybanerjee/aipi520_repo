{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN and Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a dataset from the University of Wisconsin which contains features of the cell nuclei present in biopsies of breast masses.  The target to predict is whether the mass is malignant or benign.  Description of the dataset can be found here: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data=load_breast_cancer(as_frame=True)\n",
    "X,y=data.data,data.target\n",
    "# Since the default in the file is 0=malignant 1=benign we want to reverse these\n",
    "y=(y==0).astype(int)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set aside a test set and use the remainder for training and cross-validation\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.2)\n",
    "\n",
    "# Let's scale the inputs to help it converge more easily\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train),columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test),columns=X_train.columns)\n",
    "\n",
    "# Let's create a model using just two features so we can visualize it\n",
    "X_train_2feats = X_train_scaled[['worst concave points','worst area']]\n",
    "X_test_2feats = X_test_scaled[['worst concave points','worst area']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(X,y,model):\n",
    "    \"\"\"\n",
    "    Plots the 2D decision boundary of a classification model\n",
    "    Parameters:\n",
    "    X (pandas dataframe): input features\n",
    "    y (pandas series): target values\n",
    "    model: trained scikit-learn model object\n",
    "    \"\"\"\n",
    "    markers = ['^','s','v','o','x']\n",
    "    colors = ['yellow','green','purple','blue','orange']\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    \n",
    "    for i,k in enumerate(np.unique(y)):\n",
    "        plt.scatter(X.loc[y.values==k].iloc[:,0],X.loc[y.values==k].iloc[:,1],\n",
    "                    c=colors[i],marker=markers[i],label=k,edgecolor='black')\n",
    "\n",
    "    xgrid = np.arange(X.iloc[:,0].min(),X.iloc[:,0].max(),\n",
    "                      (X.iloc[:,0].max()-X.iloc[:,0].min())/500)\n",
    "    ygrid = np.arange(X.iloc[:,1].min(),X.iloc[:,1].max(),\n",
    "                      (X.iloc[:,1].max()-X.iloc[:,1].min())/500)\n",
    "    xx,yy = np.meshgrid(xgrid,ygrid)\n",
    "    \n",
    "    mesh_preds = model.predict(np.c_[xx.ravel(),yy.ravel()])\n",
    "    mesh_preds = mesh_preds.reshape(xx.shape)\n",
    "    plt.contourf(xx,yy,mesh_preds,alpha=0.2,cmap=cmap)\n",
    "    plt.legend()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: KNN\n",
    "In this part we will use cross-validation with 5 folds and accuracy as the evaluation metric to find the optimal value for `n_neighbors`.  The search space we will evaluate for n_neighbors is [1,3,5,10].  After you find the optimal `n_neighbors`, plot the decision boundaries of your model and calculate the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Support Vector Classifiers\n",
    "### 2.1\n",
    "We will now try a SVC on our two-feature simplified dataset.  In the cells below, create two different SVC models:  \n",
    "- SVC with a linear kernel. \n",
    "- SVC with a RBF kernel. \n",
    "\n",
    "For each model, keep the value of C fixed at 1.  Use k-folds cross-validation with k=10 to compare the performance of the two models.  Also, display the decision boundary for each model. Then, select the kernel which gives you better cross-validation performance as your final model and calculate the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2\n",
    "Now, let's try a polynomial kernel.  Vary the polynomial degree from 2 through 4 and create and train a SVC model with a polynomial kernel of each degree.  Leave the value of C constant at 1.  For each model, display the resulting decision boundary and calculate the cross-validation accuracy using k=10.  Visually compare the decision boundaries and their performance in classifying the data.  Then, determine which degree has the best performance in classifying the data and calculate the performance of a SVC model with that degree of polynomial kernel on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION ###\n",
    "\n",
    "\n",
    "\n",
    "### END SOLUTION ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('aipi540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31cc86d7aac4849c7546154c9b56d60163d5e8a1d83593a5eed18774fbf4fd37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
